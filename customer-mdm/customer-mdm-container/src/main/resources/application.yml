server:
  port: 8185

customer-service:
  permit-cross-origin-from: http://localhost:8285
  swagger-port: 8285
  customer-event:
    topic-name: customer-event

security:
  jwt:
    authority-prefix: SCOPE_
    principal-claim-name: "preferred_username"

spring:
  config:
    import: optional:file:.env[.properties]
  main:
    banner-mode: off
  jpa:
    open-in-view: false
    show-sql: false
    database-platform: org.hibernate.dialect.PostgreSQLDialect
    properties:
      default_schema: customer_master
      hibernate:
        dialect: org.hibernate.dialect.PostgreSQLDialect
  datasource:
    url: jdbc:postgresql://${DATASOURCE_HOST}:${DATASOURCE_PORT}/${DATASOURCE_DATABASE}?currentSchema=customer_master&binaryTransfer=true&reWriteBatchedInserts=true&stringType=unspecified
    username: ${DATASOURCE_USERNAME}
    password: ${DATASOURCE_PASSWORD}
    driver-class-name: org.postgresql.Driver
  sql:
    init:
      platform: postgres
      mode: always
  liquibase:
    change-log: "classpath:db/changelog/changelog-master.yaml"
    default-schema: customer_master
  security:
    oauth2:
      client:
        registration:
          keycloak:
            client-id: ticketing
            authorization-grant-type: authorization_code
            scope: openid
        provider:
          keycloak:
            issuer-uri: http://${KEYCLOAK_HOST}:${KEYCLOAK_PORT}/realms/acroteq
            user-name-attribute: preferred_username
      resourceserver:
        jwt:
          issuer-uri: http://${KEYCLOAK_HOST}:${KEYCLOAK_PORT}/realms/acroteq
  kafka:
    bootstrap-servers: ${KAFKA_BOOTSTRAP_SERVERS}
    security:
      protocol: "SSL"
    ssl:
      key-store-location: classpath:/certs/customer-mdm.keystore.jks
      key-store-password: acroteq
      trust-store-location: classpath:/certs/customer-mdm.truststore.jks
      trust-store-password: acroteq
    producer:
      acks: all
      batch-size: 16384
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: io.confluent.kafka.serializers.KafkaAvroSerializer
      compression-type: snappy
      retries: 5
      properties:
        linger.ms: 5
        schema.registry.url: ${SCHEMA_REGISTRY_URL}
        ssl.endpoint.identification.algorithm: ""
    listener:
      type: batch
      ack-mode: batch
      concurrency: 1
      poll-timeout: 150
    consumer:
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: io.confluent.kafka.serializers.KafkaAvroDeserializer
      heartbeat-interval: 3000
      max-poll-records: 500

kafka:
  consumer:
    dead-letter:
      suffix: -dlt
    backoff:
      max-retries: 3
      initial-interval-in-seconds: 10
      multiplier: 2
      max-interval-in-seconds: 40
